<!DOCTYPE html>
<!--Adapted from: https://necv2023.github.io/-->
<html lang="en">
    <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=0.67">
    <meta name="description" content="NECV 2024 website.">
    <meta name="keywords" content="NECV 2024">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <style type="text/css">
        body {
            font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
            font-weight:300;
            font-size:14px;
            margin-left: auto;
            margin-right: auto;
            width: 90%;
            text-align: justify;
            text-justify: inter-word;
            line-height: 1.5;
        }

        h1 {
            font-size:28px;
            font-weight:250;
        }

        h2 {
            font-size:24px;
        }

        h3 {
            font-size:20px;
        }

        h4 {
            font-size:17px;
        }
        .author_list {
            font-size: 10pt;
        }
        .talk_title {
            font-size: 11pt;
            color: #286DC0;
        }

        div.text-box {
            width: 95%;
            padding: 5px;
            border: 4px solid #66cc33;
            margin: 0;
        }
        
        img.header-img {
            border: 1px solid black;
            border-radius: 10px ;
            -moz-border-radius: 10px ;
            -webkit-border-radius: 10px ;
        }
        
        img.rounded {
            border: 1px solid #eeeeee;
            border-radius: 10px ;
            -moz-border-radius: 10px ;
            -webkit-border-radius: 10px ;
        }
        
        a:link,a:visited
        {
            color: #286DC0;
            text-decoration: none;
        }
        a:hover {
            color: #286DC0;
        }
        
        table { 
            border-collapse: collapse; 
            text-align: left;
        }
        table.table1 td.dl-link {
            height: 140px;
            text-align: center;
            font-size: 15px;
        }
        table.table1 tr {
          border-top: 1pt solid black;
          border-bottom: 1pt solid black;
        }
        
        img.logo {
          display: block;
          margin-left: auto;
          margin-right: auto;
        }
        
        .vert-cent {
            position: relative;
            top: 50%;
            transform: translateY(-50%);
        }
        
        hr
        {
            border: 0;
            height: 1px;
            background-image: linear-gradient(to right, rgba(0, 0, 0, 0.25), rgba(0, 0, 0, 0.25), rgba(0, 0, 0, 0.25));
        }

        .google-maps {
            width: 100%;
            max-width: 100%;
            display: block;
        }
        .google-maps iframe {
            width: 90% !important; 
            height: 250px; 
            display: block; 
        }
        
        @media only screen and (min-width: 768px) { 
            body {
                font-size:20px;
                width: 80%;
            }
                
            h1 {
                font-size:38px;
                font-weight:300;
            }
            h2 {
                font-size:32px;
            }
    
            h3 {
                font-size:28px;
            }
    
            h4 {
                font-size:24px;
            }
            .author_list {
                font-size: 12pt;
            }
            .talk_title {
                font-size: 13pt;
            }    
          	table.table1 td.dl-link {
          		height: 160px;
          		font-size: 22px;
          	}
            table.table1 tr {
              border-top: 1pt solid black;
              border-bottom: 1pt solid black;
            }

            .google-maps iframe {
                height: 400px;
            }
            
        }
    </style>

    <title>NECV 2024</title>
    </head>
    
  <body>
    
    <div class="container w-75" align="center">
        <h1><b>New England Computer Vision (NECV) Workshop 2024</b></h1>
        <h2><a href="https://www.yale.edu/" target="_blank">Yale University</a>, New Haven, CT</h2>
        <em><h2>Friday, November 22, 2024</h2></em>
    </div>

    <div class="container w-75" align="center">
        <img src="./files/yale.jpg" width="80%" style="margin: 1em 0em 1em 0em; border: 1px solid #000"><br>
    </div>

    <div class="container w-75" >
        <hr>
        The New England Computer Vision Workshop (NECV) brings together researchers in computer vision and related areas 
        for an informal exchange of ideas through a full day of presentations and posters.
        Held conveniently after the CVPR deadline and before the NeurIPS conference, NECV offers opportunities to network and showcase research.
        NECV attracts researchers from universities and industry research labs in New England.
        As in previous years, the workshop will focus on graduate student presentations.
        Welcome to Yale!
        <br>
        <p align="right">
            - <a href="https://vision.cs.yale.edu/members/alex-wong.html" target="_blank">Alex Wong</a>
        </p>
    </div>

    <div class="container w-75">
        <hr>
        <h2>Registration and Submission</h2>

        <b>Academic researchers</b>:
        Participation is free for all researchers at academic institutions.
        Please register <a href="https://docs.google.com/forms/d/e/1FAIpQLSdZTIGWfgGOQ2hcPcgJQw9U34-DoyDvDBOF61e0TcgqlEqg2Q/viewform?usp=sf_link" target="_blank">here</a>
        and submit your abstract <a href="https://docs.google.com/forms/d/e/1FAIpQLSdC_N9wWQDMZJH_gfvdDsQZ-O_Ncb3HVjUrVSe4ibtaJS4Pxg/viewform?usp=sf_link" target="_blank">here</a>.
        <br><br>
        
        <b>Industry participants</b>:
        For our industry friends, a limited number of registrations are available for a fee.
        Please register <a href="https://docs.google.com/forms/d/e/1FAIpQLSfOi3bM33UMQQMjOZ-AD4mgvkE7ovE8mmEleSfs_L9OQqkVbA/viewform?usp=sf_link" target="_blank">here</a>.
        <br><br>

        <b>Deadlines</b>:
        Early-bird registration (lunch provided) by <u>November 7</u>.
        Please register by <u>November 15</u> and submit by <u>November 17</u>.
        Oral decisions will be released by <u>November 19</u>.
        <br><br>

        <b>Submission guidelines</b>:
        Please submit a one-page PDF abstract using the <a href="https://github.com/cvpr-org/author-kit/releases/tag/CVPR2025-v3(latex)" target="_blank">CVPR 2025 rebuttal template</a>.
        Please include the title of your work and the list of authors in the abstract.

        You may present work that has already been published or work that is in progress.
        All relevant submissions will be granted a poster presentation,
        and <u>selected submissions from each institution will be granted 8-minute oral presentations</u>.
        Post-docs and faculty may submit for poster presentations, but oral presentations are reserved for graduate students.

        There will be no publications resulting from the workshop,
        so presentations will not be considered "prior peer-reviewed work" according to any definition we are aware of.
        Thus, work presented at NECV can be subsequently submitted to other venues without citation.

        The workshop is after the CVPR submission deadline, so come and show off your new work in a friendly environment.
        It's also just before the NeurIPS conference, so feel free to come and practice your presentation.
        <br><br>
    </div>

    <div class="container w-75">
        <hr>
        <h2>Presentation</h2>

        <p>
        <b>Oral presentation</b>:
        Each presentation is allocated a <u>6-minute</u> slot, with an additional <u>2 minutes</u> dedicated to questions.
        We kindly request all oral presenters to bring their machines for their presentation.
        The presentation equipment supports both HDMI and Type-C for screen sharing.
        Please arrive at least <u>5 minutes</u> before the scheduled oral session to test your machine and ensure compatibility with the provided equipment.
        Similar to regular conferences, we have also allocated poster boards for oral presenters. Please find your poster ID.
        </p>

        <p>
        <b>Poster presentation</b>:
        Please locate the correct poster board to display your poster.
        Easels and foam cores will be provided for mounting posters, accommodating sizes up to <u>36x48 inches</u>. 
        The foam cores are not attached, allowing flexibility for landscape or portrait orientation. You are welcome to use any format within that size limit.
        </p>
        
    </div>

    <div class="container w-75">
        <hr>
        <h2>Logistics</h2>
        
        <h3>Schedule</h3>
        <table class="table1">
            <tbody>
            <tr bgcolor="#E5E8E8">
                <td height=40px><b>Time</b></td>
                <td><b>Topic</b></td>
            </tr>
            <tr>
                <td height=40px>9:00-10:00</td>
                <td>Registration & Poster Setup</td>
            </tr>
            <tr>
                <td height=40px>10:00-10:10</td>
                <td>Welcome & Opening</td>
            </tr>
            <tr bgcolor="#FEF9E7">
                <td height=40px>10:15-11:15</td>
                <td><b>Oral Session I</b></td>
            </tr>
            <tr bgcolor="#FEF9E7">
                <td colspan="2">
                    <ul>
                        <li>
                            <span class="talk_title">[10:15] Learning to Edit Visual Programs with Self-Supervision</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[10:25] What if Eye? Computationally Emulating the Evolution of Visual Intelligence</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[10:35] Orient Anything</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[10:45] Score Distillation via Reparameterized DDIM</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[10:55] Straightening Flow Matching Models by Learning Interpolants</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[11:05] Time of the Flight of the Gaussians:Fast and Accurate Dynamic Time-of-Flight Radiance Fields</span> <br>
                        </li>
                    </ul>
                </td>
            </tr>
            <tr bgcolor="#FEF9E7">
                <td height=40px>11:25-12:25</td>
                <td><b>Poster Session I</b></td>
            </tr>
            <tr bgcolor="#FEF9E7">
                <td colspan="2">
                    <ul>
                        <li>
                            <span class="talk_title">[1] Augundo: Scaling up augmentations for monocular depth completion and estimation</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[2] Orient Anything</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[3] What if Eye? Computationally Emulating the Evolution of Visual Intelligence</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[4] Enhancing Autonomous Navigation by Imaging Hidden Objects using Single-Photon LiDAR</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[5] Active Appearance and Spatial Variation Can Improve Visibility in Area Labels for Augmented Reality</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[6] E-BARF: Bundle Adjusting Neural Radiance Fields from a Moving Event Camera</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[7] CUTS: A Deep Learning and Topological Framework for Multigranular Unsupervised Medical Image Segmentation</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[8] GigaHands: A Massive Annotated Dataset of Bimanual Hand Activities</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[9] Real-Time Temporally Consistent Depth Completion for VR-Teleoperated Robots</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[10] OneGaze: A Unified Model for Estimating Gazeing Egocentric Videos and Still Images</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[11] Audio Geolocation: An Investigation with Natural Sounds</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[12] Straightening Flow Matching Models by Learning Interpolants</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[13] CP-TRPCA: A Novel Approach to Robust Tensor PCA</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[14] ODTFormer: Efficient Stereo-based Obstacle Detection with Deformable Matching Cost Attention</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[15] Text-Aware Diffusion for Policy Learning</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[16] Hybrid CNN with Multimodal Data for Early Alzheimer's Disease Forecasting</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[17] Combining Observational Data and Language for Species Range Estimation</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[18] Learning to Edit Visual Programs with Self-Supervision</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[19] Time of the Flight of the Gaussians:Fast and Accurate Dynamic Time-of-Flight Radiance Fields</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[20] Personalized Representation from Personalized Generation</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[21] Audio-Visual Speech Separation via Bottleneck Iterative Network</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[22] Score Distillation via Reparameterized DDIM</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[23] Pre-trained Vision-Language Models Learn Discoverable Visual Concepts</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[24] Adaptive Correspondence Scoring for Unsupervised Medical Image Registration</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[25] Mine yOur owN Anatomy: Revisiting Medical Image Segmentation with Extremely Limited Labels</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[26] ProtoDepth: Unsupervised Continual Depth Completion with Prototypes</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[27] RSA: Resolving scale ambiguities in monocular depth estimators through language descriptions</span> <br>
                        </li>
                    </ul>
                </td>
            </tr>
            <tr>
                <td height=40px>12:30-1:30</td>
                <td>Lunch</td>
            </tr>
            <tr bgcolor="#FEF9E7">
                <td height=40px>1:50-2:40</td>
                <td><b>Oral Session II</b></td>
            </tr>
            <tr bgcolor="#FEF9E7">
                <td colspan="2">
                    <ul>
                        <li>
                            <span class="talk_title">[1:50] Event fields: Capturing light fields at high speed, resolution, and dynamic range</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[2:00] RICH: Non-stochastic Robust Inference of Camera Headings</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[2:10] DELTA: Dense Efficient Long-range 3D Tracking for any video</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[2:20] Smooth Spline: Provably Smoothing the Decision Manifolds of Deep Neural Networks After Training</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[2:30] The GAN is dead; long live the GAN! A Modern GAN Baseline</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[2:40] The iNaturalist Sounds Dataset</span> <br>
                        </li>
                    </ul>
                </td>
            </tr>
            <tr bgcolor="#FEF9E7">
                <td height=40px>2:55-3:55</td>
                <td><b>Poster Session II</b></td>
            </tr>
            <tr bgcolor="#FEF9E7">
                <td colspan="2">
                    <ul>
                        <li>
                            <span class="talk_title">[1] WildSAT: Learning Satellite Image Representations from Wildlife Observations</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[2] Monocular Dynamic Gaussian Splatting is Fast and Brittle but Smooth Motion Helps</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[3] ImageFlowNet: Forecasting Multiscale Image-Level Trajectories of Disease Progression with Irregularly-Sampled Longitudinal Medical Images</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[4] RICH: Non-stochastic Robust Inference of Camera Headings</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[5] HouseCrafter: Lifting Floorplans to 3D Scenes with 2D Diffusion Models</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[6] Test-time adaptation for depth completion</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[7] Smooth Spline: Provably Smoothing the Decision Manifolds of Deep Neural Networks After Training</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[8] Differentiable Robot Rendering</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[9] All-day Depth Completion</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[10] ACDIT: Interpolating Autoregressive Conditional Modeling and Diffusion Transformer</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[11] DiffKillR: Killing and Recreating Diffeomorphisms for Cell Annotation in Microscopy Images</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[12] Benchmarking Single-Positive Multi-Label Methods using the L48S Dataset</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[13] UnCLe: Unsupervised Continual Learning of Depth Completion</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[14] The iNaturalist Sounds Dataset</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[15] Solving New Tasks by Adapting Internet Video Knowledge</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[16] Evidential Neural Radiance Fields</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[17] Event fields: Capturing light fields at high speed, resolution, and dynamic range</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[18] As-Ortho-As-Possible View Selection for Decomposing Appearance in Scene Reconstruction</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[19] Similarity Group Equivariant Convolutional Networks</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[20] Zero-Shot Monocular Scene Flow Estimation in the Wild</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[21] Vessel-aware aneurysm detection using multi-scale deformable 3D attention</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[22] DELTA: Dense Efficient Long-range 3D Tracking for any video</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[23] Studying Priming Effect in Vision-Language Models</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[24] Thinking in Space: How Multimodal Large Language Models See, Remember and Recall Spaces</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[25] Squeezing Water from a Stone: Improving Pre-Trained Self-Supervised Embeddings Through Effective Entropy Maximization</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[26] Compositional Content Recommendation and Controllable Object Placement with Multimodal LLM</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[27] iNatator: Obtaining Expert Feedback on Species Ranges</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[28] NeST: Neural Stress Tensor Tomography by leveraging 3D Photoelasticity</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[29] How Can Objects Help Video-Language Understanding?</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[30] Token-Shuffle: Towards High-Resolution Image Generation with Autoregressive Models</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[31] Image Reconstruction from Readout-Multiplexed Single-Photon Detector Arrays</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[32] The GAN is dead; long live the GAN! A Modern GAN Baseline</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[33] Generate, Transduct, Adapt: Iterative Transduction with VLMs</span> <br>
                        </li>
                    </ul>
                </td>
            </tr>
            <tr>
                <td height=40px>3:55-4:25</td>
                <td>Coffee Break</td>
                <td></td>
            </tr>
            <tr>
                <td height=40px>4:30-4:40</td>
                <td>Statement by <a href="https://jeffbrock.net/about/index.html" target="_blank">Jeffrey Brock</a>, Dean of Yale SEAS</td>
                <td></td>
            </tr>
            <tr bgcolor="#FEF9E7">
                <td height=40px>4:40-6:10</td>
                <td><b>Oral Session III</b></td>
            </tr>
            <tr bgcolor="#FEF9E7">
                <td colspan="2">
                    <ul>
                        <li>
                            <span class="talk_title">[4:40] NeST: Neural Stress Tensor Tomography by leveraging 3D Photoelasticity</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[4:50] Image Reconstruction from Readout-Multiplexed Single-Photon Detector Arrays</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[5:00] Differentiable Robot Rendering</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[5:10] ACDIT: Interpolating Autoregressive Conditional Modeling and Diffusion Transformer</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[5:20] Vessel-aware aneurysm detection using multi-scale deformable 3D attention</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[5:30] Squeezing Water from a Stone: Improving Pre-Trained Self-Supervised Embeddings Through Effective Entropy Maximization</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[5:40] Solving New Tasks by Adapting Internet Video Knowledge</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[5:50] Generate, Transduct, Adapt: Iterative Transduction with VLMs</span> <br>
                        </li>
                        <li>
                            <span class="talk_title">[6:00] ImageFlowNet: Forecasting Multiscale Image-Level Trajectories of Disease Progression with Irregularly-Sampled Longitudinal Medical Images</span> <br>
                        </li>
                    </ul>
                </td>
            </tr>
            <tr>
                <td height=40px>6:10-6:20</td>
                <td>Closing Remarks</td>
            </tr> 
            </tbody>
        </table>

        <h3>Venue</h3>
        The workshop will be held at <a href="https://maps.app.goo.gl/JvgYhfZevaY5pvp18" target="_blank">Marsh Auditorium, Yale Science Building, Yale University</a>.

        <h3>WiFi</h3>
        Log on to "YaleGuest" and accept the terms.
        
        <h3>Hotel Accommodation</h3>
        <ul>
            <li><a href="https://www.hilton.com/en/hotels/hvnsdup-hotel-marcel-new-haven/" target="_blank">Hotel Marcel</a></li>
            <li><a href="https://www.hilton.com/en/hotels/wthvnhx-hampton-suites-new-haven-south-west-haven/" target="_blank">Hampton Inn & Suites New Haven - South - West Haven</a></li>
            <li><a href="https://www.hilton.com/en/hotels/hvnorhw-homewood-suites-orange-new-haven/" target="_blank">Homewood Suites by Hilton - Orange / New Haven</a></li>
            <li><a href="https://www.marriott.com/en-us/hotels/hvnco-courtyard-new-haven-orange-milford/overview/" target="_blank">Courtyard by Marriott New Haven Orange/Milford</a></li>
            <li><a href="https://www.marriott.com/en-us/hotels/hvnrh-residence-inn-new-haven-hamden/overview/" target="_blank">Residence Inn by Marriott New Haven / Hamden</a></li>
        </ul>

        <h3>Contact</h3>
        For any questions related to our upcoming workshop, please contact our committee at <a href="necv2024yale@gmail.com" target="_blank">necv2024yale@gmail.com</a>.
    </div>

    <!-- <div class="container w-75">
        <hr>
        <h2>Sponsorship</h2>
        <p>
            <a href="https://research.google/" target="_blank">
                <img class="logo" src="./files/Google_2015_logo.svg" width="20%">
            </a>
        </p>
    </div> -->

    <div class="container w-75">
        <hr>
        <h2>Organizers</h2>
            <p>
                <b>Host</b>:
                <a href="https://vision.cs.yale.edu/members/alex-wong.html" target="_blank">Alex Wong</a>.
            <p>
                <b>Program committee</b>:
                <a href="https://www.linkedin.com/in/hyoungseob-park-00692a188/" target="_blank">Hyoungseob Park</a>,
                <a href="https://adonis-galaxy.github.io/homepage/" target="_blank">Ziyao Zeng</a>,
                <a href="https://vision.cs.yale.edu/members/daniel-wang.html" target="_blank">Daniel Wang</a>,
                <a href="https://patrickqrim.github.io/" target="_blank">Patrick Rim</a>,
                <a href="https://fuzzythecat.github.io/" target="_blank">Younjoon Chung</a>,
                <a href="https://vision.cs.yale.edu/members/ruxiao-duan.html">Ruxiao Duan</a>,
                <a href="https://xiaoranzhang.com/" target="_blank">Xiaoran Zhang</a>,
                <a href="https://www.linkedin.com/in/ezhovv/" target="_blank">Vadim Ezhov</a>,
                <a href="https://www.linkedin.com/in/jihe-he/" target="_blank">Nick He</a>,
                <a href="https://www.linkedin.com/in/xe-chen/" target="_blank">Xien Chen</a>,
                <a href="https://www.sandipan.com/suchisrit.html" target="_blank">Rit Gangopadhyay</a>,
                and
                <a href="https://jingchengni.com/" target="_blank">Jingcheng Ni</a>.
            </p>
            <p>
                <b>Logistics committee</b>:
                <a href="https://www.linkedin.com/in/hyoungseob-park-00692a188/" target="_blank">Hyoungseob Park</a>,
                <a href="https://www.linkedin.com/in/ezhovv/" target="_blank">Vadim Ezhov</a>,
                and
                <a href="https://www.sandipan.com/suchisrit.html" target="_blank">Rit Gangopadhyay</a>.
            </p>
            <p>
                <b>Corporate relations committee</b>:
                <a href="https://patrickqrim.github.io/" target="_blank">Patrick Rim</a>,
                <a href="https://www.rjchen.site/" target="_blank">Runjian Chen</a>,
                <a href="https://www.linkedin.com/in/jihe-he/" target="_blank">Nick He</a>,
                and
                <a href="https://www.linkedin.com/in/xe-chen/" target="_blank">Xien Chen</a>.
            </p>
            <p>
                <b>Website chair</b>:
                <a href="https://vision.cs.yale.edu/members/ruxiao-duan.html">Ruxiao Duan</a>.
            </p>
            <p>
                <b>Steering committee</b>:
                Subhransu Maji (UMass Amherst),
                Erik Learned-Miller (UMass Amherst),
                Kate Saenko (Boston University),
                Yun (Raymond) Fu (Northeastern University),
                Octavia Camps (Northeastern University),
                Todd Zickler (Harvard),
                James Tompkin (Brown),
                Benjamin Kimia (Brown),
                Phillip Isola (MIT),
                Pulkit Agrawal (MIT),
                SouYoung Jin (Dartmouth),
                Adithya Pediredla (Dartmouth),
                and
                Yu-Wing Tai (Dartmouth).
            </p>
    </div>

    <div class="container w-75">
        <hr>
        <h2>Acknowledgements</h2>
        <p>
            We thank <a href="https://people.csail.mit.edu/samson/">Samson Timoner</a> for helping us arrange NECV 2024.
        </p>
    </div>

    <div class="container w-75"></div>
        <hr>
        <h2>Past Years</h2>
        <ul>
            <li>2023 - <a href="https://necv2023.github.io/" target="_blank">Dartmouth College</a></li>
            <li>2022 - <a href="https://necv2022.github.io/" target="_blank">Massachusetts Institute of Technology</a></li>
            <li>2019 - <a href="https://visual.cs.brown.edu/workshops/necv2019/" target="_blank">Brown University</a></li>
            <li>2018 - <a href="https://projects.iq.harvard.edu/necv2018/" target="_blank">Harvard University</a></li>
            <li>2017 - <a href="https://web.northeastern.edu/smilelab/necv2017/index.html" target="_blank">Northeastern University</a></li>
            <li>2016 - <a href="http://vision.cs.uml.edu/necv2016.html" target="_blank">Boston University</a></li>
            <li>2015 - <a href="https://people.cs.umass.edu/~smaji/nevm2015/" target="_blank">University of Massachusetts Amherst</a></li>
        </ul>
    </div>
</body></html>
